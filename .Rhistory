model_test$weight <- 1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% xgb_vars]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in% xgb_vars]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]] - 1,
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]] - 1,
weight=test_for_xgb[["weight"]])
train_for_xgb[["Default"]]
head(train_for_xgb)
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% xgb_vars & "Default"]))
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% xgb_vars && "Default"]))
colnames(train_for_xgb)
head(train_for_xgb)
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
head(train_for_xgb)
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
model_train$weight <- 1
model_test$weight <- 1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]] - 1,
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]] - 1,
weight=test_for_xgb[["weight"]])
model_train <- model.matrix(model_train)
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]] - 1,
weight=train_for_xgb[["weight"]])
model_train$weight <- 1
model_test$weight <- 1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default", "weight")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]] - 1,
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]] - 1,
weight=test_for_xgb[["weight"]])
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default", "weight")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]],
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]],
weight=test_for_xgb[["weight"]])
head(train_for_xgb$Default)
head(train_for_xgb[[Default]])
head(train_for_xgb[["Default"]])
head(train_for_xgb)
str(train_for_xgb)
str(model_train)
test_for_xgb <- model.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default", "weight")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]],
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]],
weight=test_for_xgb[["weight"]])
model_train$weight <- 1
model_test$weight <- 1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default", "weight")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_for_xgb[["Default"]],
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_for_xgb[["Default"]],
weight=test_for_xgb[["weight"]])
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = as.factor(train_for_xgb[["Default"]]),
weight=train_for_xgb[["weight"]])
train_label
test_ddd
train_label <- as.factor(model_train$Default)
test_label <- as.factor(model_test$Default)
train_label <- as.factor(model_train$Default)
test_label <- as.factor(model_test$Default)
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default", "weight")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default", "weight")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label,
weight=train_for_xgb[["weight"]])
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label,
weight=test_for_xgb[["weight"]])
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
# model_train$weight <- 1
# model_test$weight <- 1
train_label <- as.factor(model_train$Default)
test_label <- as.factor(model_test$Default)
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
dtest
str(dtest)
?roc.curve
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 10, subsample = 1, colsample_bytree = 1)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10, maximize = F)
train_label <- as.factor(model_train$Default-1)
test_label <- as.factor(model_test$Default-1)
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
train_label <- as.numeric(as.factor(model_train$Default-1))
test_label <- as.numeric(as.factor(model_test$Default-1))
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 10, subsample = 1, colsample_bytree = 1)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10, maximize = F)
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 10, subsample = 1, colsample_bytree = 1)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
#                         allowParallel = T)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 10, subsample = 10, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
#                         allowParallel = T)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 10, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = c(10,40), subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
params <- list(booster = "gbtree", objective = "reg:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
set.seed(100)
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in%  c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print.every.n = 10
,early.stop.round = 10
,maximize = F
,eval_metric = "error"
)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print_every_n = 10
,early_stopping_round = 10
,maximize = F
,eval_metric = "error"
)
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
confusionMatrix(xgbpred, test_label)
## Calculate ROC
roc.curve(model_train$Default,xgbpred)
## Calculate ROC
roc.curve(model_test$Default,xgbpred)
varImp(xgb1)
mat <- xgb.importance(feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance(importance_matrix = mat[1:20]) #first 20 variables
mat <- xgb.importance(feature_names = colnames(model_train),model = xgb1)
xgb.plot.importance(importance_matrix = mat[1:20]) #first 20 variables
mat <- xgb.importance(feature_names = colnames(model_train),model = xgb1)
xgb.plot.importance(importance_matrix) #first 20 variables
mat <- xgb.importance(feature_names = colnames(model_train),model = xgb1)
xgb.plot.importance(mat) #first 20 variables
## Variable Importance Graph
mat <- xgb.importance(feature_names = colnames(model_train),model = xgb1)
xgb.plot.importance(mat)
xgb_vars <-  c("mean_payment", "number_payments", "max_payment", "
min_payment", "median_payment", "reversedPayment", "noPayments", "
mean_balance", "mean_cash_balance", "max_balance", "max_cash_balance", "
max_num_cmp", "count_num_cmp", "credit_change", "TotalBalanceOL_perc_max", "
TotalBalanceOL_perc_mean", "TotalBalanceOL_ind", "CB_ind", "
CB_limit_perc_max", "CB_limit_perc_mean", "Spending_mean", "
SpendingOL_perc_max", "SpendingOL_perc_mean", "SpendingOL_ind")
## Creating formula for models post correlation analysis
formula <- as.formula(paste("Default ~", paste(xgb_vars, collapse = "+")))
## Formatting tables for xgboost
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
set.seed(100)
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in% c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print_every_n = 10
,early_stopping_round = 10
,maximize = F
,eval_metric = "error"
)
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
confusionMatrix(xgbpred, test_label)
## Calculate ROC
roc.curve(model_test$Default,xgbpred)
## Variable Importance Graph
mat <- xgb.importance(feature_names = colnames(model_train),model = xgb1)
xgb.plot.importance(mat)
## Variable Importance Graph
mat <- xgb.importance(feature_names = colnames(dtrain),model = xgb1)
xgb.plot.importance(mat)
## Formatting tables for xgboost
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
set.seed(100)
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(xgb_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in% c(xgb_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print_every_n = 10
,early_stopping_round = 10
,maximize = F
,eval_metric = "error"
)
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
confusionMatrix(xgbpred, test_label)
## Calculate ROC
roc.curve(model_test$Default,xgbpred)
## Variable Importance Graph
mat <- xgb.importance(feature_names = colnames(dtrain),model = xgb1)
xgb.plot.importance(mat)
revised_vars <- c("TotalBalanceOL_perc_max", "count_num_cmp", "CB_limit_perc_mean",
"median_payment", "number_payments", "max_balance", "max_cash_balance",
"credit_change")
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
set.seed(100)
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(revised_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in% c(revised_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print_every_n = 10
,early_stopping_round = 10
,maximize = F
,eval_metric = "error"
)
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
confusionMatrix(xgbpred, test_label)
## Calculate ROC
roc.curve(model_test$Default,xgbpred)
## Variable Importance Graph
mat <- xgb.importance(feature_names = colnames(dtrain),model = xgb1)
xgb.plot.importance(mat)
confusionMatrix(xgbpred, test_label)
table(billing_train$DelqCycle)
##====================================
## Logistic Regression (ROC = 0.787)
##====================================
formula_logreg <- as.formula(paste("Default ~", paste(revised_vars,collapse = "+")))
logreg_classifier <- glm(formula, family = binomial, data = model_train)
pred_logreg <- predict(logreg_classifier, newdata = model_test, type = "response")
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
pred_logreg <- predict(logreg_classifier, newdata = model_test)
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
pred_logreg <- predict(logreg_classifier, newdata = model_test, type = "response")
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
## Formatting tables for xgboost
revised_vars <- c("TotalBalanceOL_perc_max", "count_num_cmp", "CB_limit_perc_mean",
"median_payment", "number_payments", "max_balance", "max_cash_balance",
"credit_change")
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
set.seed(100)
train_label <- as.numeric(as.factor(model_train$Default))-1
test_label <- as.numeric(as.factor(model_test$Default))-1
train_for_xgb <- as.matrix(copy(model_train[,colnames(model_train) %in% c(revised_vars,"Default")]))
test_for_xgb <- as.matrix(copy(model_test[,colnames(model_test) %in% c(revised_vars,"Default")]))
dtrain <- xgb.DMatrix(data = train_for_xgb,
label = train_label)
dtest <- xgb.DMatrix(data = test_for_xgb,
label = test_label)
## Keeping the model under control
params <- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma = 0,
max_depth = 6, min_child_weight = 50, subsample = 1, colsample_bytree = 0.8)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T,
print_every_n = 5, early_stopping_rounds = 10)
xgb1 <- xgb.train(params = params
,data = dtrain
,nrounds = 25
,watchlist = list(val=dtest,train=dtrain)
,print_every_n = 10
,early_stopping_round = 10
,maximize = F
,eval_metric = "error"
)
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
confusionMatrix(xgbpred, test_label)
## Calculate ROC
roc.curve(model_test$Default,xgbpred)
pred_logreg <- predict(logreg_classifier, newdata = model_test, type = "response")
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
head(performance_test)
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred_rounded <- ifelse(final_pred >= 0.3,1,0)
submission <- data.frame(test$ID_CPTE, test$xgb_pred)
colnames(submission) = c("ID_CPTE", "Default")
colnames(test)
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
formula_logreg <- as.formula(paste("Default ~", paste(revised_vars,collapse = "+")))
logreg_classifier <- glm(formula, family = binomial, data = model_train)
pred_logreg <- predict(logreg_classifier, newdata = model_test, type = "response")
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred_rounded <- ifelse(final_pred >= 0.3,1,0)
submission <- data.frame(test$ID_CPTE, test$xgb_pred)
colnames(submission) = c("ID_CPTE", "Default")
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
train[[cn]] <- as.numeric(train[[cn]])
test[[cn]] <- as.numeric(test[[cn]])
}
## Spliting Data for models
library(caTools)
set.seed(8)
split = sample.split(train$Default, SplitRatio = 0.70)
model_train = subset(train, split == TRUE)
model_test = subset(train, split == FALSE)
##====================================
## Logistic Regression (ROC = 0.798)
##====================================
logi_vars <- c("credit_change", "reversedPayment", "noPayments")
for (cn in logi_vars){
model_train[[cn]] <- as.numeric(model_train[[cn]])
model_test[[cn]] <- as.numeric(model_test[[cn]])
}
formula_logreg <- as.formula(paste("Default ~", paste(revised_vars,collapse = "+")))
logreg_classifier <- glm(formula, family = binomial, data = model_train)
pred_logreg <- predict(logreg_classifier, newdata = model_test, type = "response")
pred_rounded_logreg <- ifelse(pred_logreg >= 0.30,1,0)
## Calculate ROC
roc.curve(model_test$Default, pred_rounded_logreg)
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred_rounded <- ifelse(final_pred >= 0.3,1,0)
submission <- data.frame(test$ID_CPTE, test$xgb_pred)
colnames(submission) = c("ID_CPTE", "Default")
test$logreg_pred <- final_pred_rounded
submission <- data.frame(test$ID_CPTE, test$logreg_pred)
colnames(submission) = c("ID_CPTE", "Default")
head(submission)
head(performance_test)
submission_final = merge(performance_test, submission, by = "ID_CPTE", all.x = TRUE)
head(submission_final)
submission_final = merge(performance_test, submission, by = "ID_CPTE")
head(submission_final)
submission_final = merge(submission, performance_test, by = "ID_CPTE")
head(submission_final)
performance_test <- as.data.table(read.csv(paste0(getwd(),"/Data/performance_test.csv")))
## WD
setwd("~/Github/DesjardinsDataCup/")
performance_test <- as.data.table(read.csv(paste0(getwd(),"/Data/performance_test.csv")))
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred_rounded <- ifelse(final_pred >= 0.3,1,0)
test$logreg_pred <- final_pred_rounded
submission <- data.frame(test$ID_CPTE, test$logreg_pred)
colnames(submission) = c("ID_CPTE", "Default")
submission_final = merge(performance_test, submission, by = "ID_CPTE")
head(submission_final)
performance_test <- as.data.table(read.csv(paste0(getwd(),"/Data/performance_test.csv")))
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred_rounded <- ifelse(final_pred >= 0.3,1,0)
test$logreg_pred <- final_pred_rounded
submission <- data.frame(test$ID_CPTE, test$logreg_pred)
colnames(submission) = c("ID_CPTE", "Default")
submission_final = merge(submission,performance_test, by = "ID_CPTE")
head(submission_final)
table(submission_final$Default.x)
table(submission_final$Default.y)
submission
write.csv(submission,paste0(getwd(),"/Submissions/submission_1_JUL10.csv"))
final_pred <- predict(logreg_classifier, newdata = test, type = "response")
final_pred
plot(final_pred)
hist(final_pred)
submission <- data.frame(test$ID_CPTE, test$final_pred)
test$final_pred <- final_pred
submission <- data.frame(test$ID_CPTE, test$final_pred)
colnames(submission) = c("ID_CPTE", "Default")
write.csv(submission,paste0(getwd(),"/Submissions/submission_3_JUL10_probs.csv"))
formula
